{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP_Numpy_FashionMNIST.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrtnMndt/mlpr19/blob/master/week5/MLP_Numpy_FashionMNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "543bFdMkKHJD",
        "colab_type": "text"
      },
      "source": [
        "# Classification of fashion items in images\n",
        "\n",
        "Zalando research has open-sourced a dataset for classification of 10 different categories of fashion items: https://github.com/zalandoresearch/fashion-mnist\n",
        "\n",
        "The dataset is referred to as \"Fashion-MNIST\" as a reference to the much older but still very popular MNIST dataset. The rationale behind this reference is that the dataset has similar properties. It can thus be used in the exact same manner as MNIST has been used for years of research.\n",
        "\n",
        "Specifically, we are dealing with 28x28 grayscale images from the following classes:\n",
        "\n",
        "    0 T-shirt/top\n",
        "    1 Trouser\n",
        "    2 Pullover\n",
        "    3 Dress\n",
        "    4 Coat\n",
        "    5 Sandal\n",
        "    6 Shirt\n",
        "    7 Sneaker\n",
        "    8 Bag\n",
        "    9 Ankle boot\n",
        "\n",
        "In comparison to the traditional handwritten-digit recognition shown in MNIST, this dataset is slightly harder (it is very easy to achieve more than 99% accuracy on MNIST with only subtleties in performance based on methodology). On Fashion-MNIST we will see a much bigger difference when moving from e.g. a MLP to a deep CNN later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8nxkF6zKaWv",
        "colab_type": "text"
      },
      "source": [
        "## MLP from scratch - using Numpy only\n",
        "In order to learn more about neural networks we will start with an example of a 1 (or if you want 2) hidden layer Multilayer Perceptron (MLP) from scratch implemented with Numpy only. While there is many novel libraries that abstract away all the math and algorithms, it is a good practice to go through the basics in detail once before moving on to the more complex CNN or RNN examples.\n",
        "\n",
        "### Getting the data\n",
        "By now you should have a fair grasp of how to write a data-loader so we are not going to implement a data-loader from scratch again. The official repository for Fashion-MNIST already has instructions of how to load the data. However it's sort of inconvenient given that you have to download files by hand, unpack them, call their API etc.\n",
        "\n",
        "In general, what frameworks such as TensorFlow and PyTorch will later do for us is write a dataset class that offers all the necessary functionality for us. This usually includes automatical downloads (and checks to not download if the data is already existent), offer functions to get training and validation splits, possibly transformations or datatype options, or even a complete data-loader including suitable iterators.\n",
        "\n",
        "We have given a minimal example of what such a data-loader could look like below. For now, you should examine it and try to understand the individual components. When we move to the next session where we will implement a CNN with a modern Deep Learning framework such as PyTorch, we will see how easily we could extend this data-loader to be included there. Of course the PyTorch developers have already done that for us, but we will go through the practice nevertheless. This way you can learn how to contribute to modern Deep Learning software and potentially even do a pull-request of your own dataset or some other non-wrapped dataset at some point in time.\n",
        "\n",
        "### Dataset class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iI1QwoscJlD6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import struct\n",
        "import gzip\n",
        "import errno\n",
        "import numpy as np\n",
        "\n",
        "class FashionMNIST:\n",
        "    \"\"\"\n",
        "    Fashion MNIST dataset featuring gray-scale 28x28 images of\n",
        "    fashion items belonging to ten different classes.\n",
        "    Dataloader adapted from MNIST.\n",
        "    We do not define __getitem__ and __len__ in this class\n",
        "    as we are using torch.utils.data.TensorDataSet which\n",
        "    already implements these methods.\n",
        "\n",
        "    Attributes:\n",
        "        train_x (np.array): Training set images.\n",
        "        train_y (np.array): Training set labels.\n",
        "        val_x (np.array): Validation set images.\n",
        "        val_y (np.array): Validation set labels.\n",
        "        \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.path = os.path.expanduser('datasets/FashionMNIST')\n",
        "        self.__download()\n",
        "\n",
        "        self.train_x, self.train_y, self.val_x, self.val_y = self.get_dataset()\n",
        "\n",
        "        self.class_to_idx = {'T-shirt/top': 0,\n",
        "                        'Trouser': 1,\n",
        "                        'Pullover': 2,\n",
        "                        'Dress': 3,\n",
        "                        'Coat': 4,\n",
        "                        'Sandal': 5,\n",
        "                        'Shirt': 6,\n",
        "                        'Sneaker': 7,\n",
        "                        'Bag': 8,\n",
        "                        'Ankle boot': 9}\n",
        "\n",
        "    def __check_exists(self):\n",
        "        \"\"\"\n",
        "        Checks if dataset has already been downloaded\n",
        "\n",
        "        Returns:\n",
        "             bool: True if downloaded dataset has been found\n",
        "        \"\"\"\n",
        "\n",
        "        return os.path.exists(os.path.join(self.path, 'train-images-idx3-ubyte.gz')) and \\\n",
        "               os.path.exists(os.path.join(self.path, 'train-labels-idx1-ubyte.gz')) and \\\n",
        "               os.path.exists(os.path.join(self.path, 't10k-images-idx3-ubyte.gz')) and \\\n",
        "               os.path.exists(os.path.join(self.path, 't10k-labels-idx1-ubyte.gz'))\n",
        "\n",
        "    def __download(self):\n",
        "        \"\"\"\n",
        "        Downloads the Fashion-MNIST dataset from the web if dataset\n",
        "        hasn't already been downloaded.\n",
        "        \"\"\"\n",
        "\n",
        "        from six.moves import urllib\n",
        "\n",
        "        if self.__check_exists():\n",
        "            return\n",
        "\n",
        "        print(\"Downloading FashionMNIST dataset\")\n",
        "        urls = [\n",
        "            'https://cdn.rawgit.com/zalandoresearch/fashion-mnist/ed8e4f3b/data/fashion/train-images-idx3-ubyte.gz',\n",
        "            'https://cdn.rawgit.com/zalandoresearch/fashion-mnist/ed8e4f3b/data/fashion/train-labels-idx1-ubyte.gz',\n",
        "            'https://cdn.rawgit.com/zalandoresearch/fashion-mnist/ed8e4f3b/data/fashion/t10k-images-idx3-ubyte.gz',\n",
        "            'https://cdn.rawgit.com/zalandoresearch/fashion-mnist/ed8e4f3b/data/fashion/t10k-labels-idx1-ubyte.gz',\n",
        "        ]\n",
        "\n",
        "        # download files\n",
        "        try:\n",
        "            os.makedirs(self.path)\n",
        "        except OSError as e:\n",
        "            if e.errno == errno.EEXIST:\n",
        "                pass\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "        for url in urls:\n",
        "            print('Downloading ' + url)\n",
        "            data = urllib.request.urlopen(url)\n",
        "            filename = url.rpartition('/')[2]\n",
        "            file_path = os.path.join(self.path, filename)\n",
        "            with open(file_path, 'wb') as f:\n",
        "                f.write(data.read())\n",
        "\n",
        "        print('Done!')\n",
        "\n",
        "    def __get_fashion_mnist(self, path, kind='train'):\n",
        "        \"\"\"\n",
        "        Load Fashion-MNIST data\n",
        "\n",
        "        Parameters:\n",
        "            path (str): Base directory path containing .gz files for\n",
        "                the Fashion-MNIST dataset\n",
        "            kind (str): Accepted types are 'train' and 't10k' for\n",
        "                training and validation set stored in .gz files\n",
        "\n",
        "        Returns:\n",
        "            numpy.array: images, labels\n",
        "        \"\"\"\n",
        "\n",
        "        labels_path = os.path.join(path,\n",
        "                                   '%s-labels-idx1-ubyte.gz'\n",
        "                                   % kind)\n",
        "        images_path = os.path.join(path,\n",
        "                                   '%s-images-idx3-ubyte.gz'\n",
        "                                   % kind)\n",
        "\n",
        "        with gzip.open(labels_path, 'rb') as lbpath:\n",
        "            struct.unpack('>II', lbpath.read(8))\n",
        "            labels = np.frombuffer(lbpath.read(), dtype=np.uint8)\n",
        "\n",
        "        with gzip.open(images_path, 'rb') as imgpath:\n",
        "            struct.unpack(\">IIII\", imgpath.read(16))\n",
        "            images = np.frombuffer(imgpath.read(), dtype=np.uint8).reshape(len(labels), 784)\n",
        "\n",
        "        return images, labels\n",
        "\n",
        "\n",
        "    def get_dataset(self):\n",
        "        \"\"\"\n",
        "        Loads and wraps training and validation datasets\n",
        "\n",
        "        Returns:\n",
        "             np.array: x_train, y_train, x_val, y_val\n",
        "        \"\"\"\n",
        "\n",
        "        x_train, y_train = self.__get_fashion_mnist(self.path, kind='train')\n",
        "        x_val, y_val = self.__get_fashion_mnist(self.path, kind='t10k')\n",
        "\n",
        "        return x_train, y_train, x_val, y_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JO5BXejlK8GY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Dataset = FashionMNIST()\n",
        "print(Dataset.train_x.shape, Dataset.train_y.shape, Dataset.val_x.shape, Dataset.val_y.shape)\n",
        "\n",
        "# normalize the uint8 images to a float 0-1 range. \n",
        "# This is necessary so that the Sigmoid function does not saturate and the gradient doesn't vanish\n",
        "Dataset.train_x = ...\n",
        "Dataset.val_x = ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7vqJjKTLBoB",
        "colab_type": "text"
      },
      "source": [
        "## Setting up a Neural Network with hidden layers: the multi-layer perceptron (MLP)\n",
        "We will take a look at a MLP where every unit of one layer is connected to every unit of the next with multiple hidden layers.\n",
        "\n",
        "If we recall a logistic regression, then this could be interpreted as just a perceptron, i.e. a very simple one layer neural network that maps the input through an activation function to the output. In MLPs each layer consist of such a building block.\n",
        "\n",
        "Let us therefore first define the rectified linear unit (ReLU) activation function and the Sigmoid function necessary to obtain the output probability and their respective derivatives required for backpropagation.\n",
        "\n",
        "* Sigmoid: $\\frac{1}{1 + \\exp(-x)}$\n",
        "* ReLU: $max(0, x)$\n",
        "\n",
        "For the derivative of the Sigmoid function we can use a trick by adding and subtracting from the derivative obtained by the quotient rule. This way we can obtain the derivative in a much more simple form: $x * (1 - x)$\n",
        "\n",
        "We recommend to go through the respective derivation that is nicely detailed in e.g. this blog post: https://beckernick.github.io/sigmoid-derivative-neural-network/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rasnz4sRLEuX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sigmoid function\n",
        "def sigmoid(x, deriv=False):\n",
        "    # define the derivative\n",
        "    if(deriv==True):\n",
        "        d = # the derivative\n",
        "        return d\n",
        "    return # the function\n",
        "\n",
        "# activation function: here a ReLU\n",
        "def ReLU(x, deriv=False):\n",
        "    # define the derivative\n",
        "    if (deriv == True):\n",
        "        d = # the derivative\n",
        "        return d\n",
        "    else:\n",
        "        return # the function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ud4ba3rvLNb-",
        "colab_type": "text"
      },
      "source": [
        "### Optimization with gradient descent\n",
        "Similarly to what we have already learned in logistic regression, we will run mini-batch stochastic gradient descent, i.e. an update step requires a subset of the dataset, in order to optimize the weights of our neural network.\n",
        "\n",
        "Again we can decide whether we want to train in a regression fashion or using a likelihood approach such as with the cross entropy loss. In any way we will need to change our labels from a single integer, to a one-hot-vector where an index 1 is at the position of the correct answer. In our example this will be a vector of length 10, with all zeros and exactly one one. The error can then be computed between the length 10 label/target vector and the 10 output neurons of our model. In this very simple example we will just use the mean squared error between the two.\n",
        "\n",
        "It is recommended to use the weight initialization scheme derived by He. et. al in \"Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\" https://arxiv.org/abs/1502.01852 for rectified linear units:\n",
        "\n",
        "* $W \\sim \\mathcal{N}(0, \\sqrt{\\frac{2}{\\mathtt{number \\, of \\, layer \\, units}}})$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gK4E9VWLe37",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "activation = ReLU\n",
        "\n",
        "# seed random numbers for reproducibility\n",
        "np.random.seed(10)\n",
        "\n",
        "# get the total amount of classes\n",
        "n_classes = np.max(Dataset.train_y) + 1\n",
        "\n",
        "# set amount of hidden units for the two hidden layers. A good starting value is 100\n",
        "h1_units = 100\n",
        "h2_units = 100\n",
        "\n",
        "\n",
        "# Initialize the sets of weights for each layer\n",
        "W0 = #\n",
        "W1 = #\n",
        "W2 = #\n",
        "\n",
        "# Create a one-hot target encoding\n",
        "# empty one-hot matrix\n",
        "y = np.zeros((Dataset.train_y.shape[0], n_classes))\n",
        "\n",
        "# set target idx to 1\n",
        "y[...] = 1\n",
        "\n",
        "# choose a batch size and learning rate. Good initial values are a \n",
        "# mini-batch size of 100 and a small learning rate of 10e-5\n",
        "mb_size = 100\n",
        "lr = 0.00001\n",
        "\n",
        "epochs = 20\n",
        "# optimize\n",
        "for epoch in range(epochs):\n",
        "    \n",
        "    # shuffle the dataset\n",
        "    perm = # create indices\n",
        "    x_train, y_train = # index the data and labels (be careful to index your newly created y and not Dataset.train_y)\n",
        "    \n",
        "    # because we are shuffling the dataset at every point and we do not want\n",
        "    # updates on a tiny batch size we can neglect the last mini-batch that is \n",
        "    # smaller than our mini-batch nice. \n",
        "    for mb in range(int(len(x_train) / mb_size)):\n",
        "        # select the dataset mini-batch subset for data and labels\n",
        "        inp = x_train[...]\n",
        "        target = y_train[...]\n",
        "    \n",
        "        # calculate the forward propagation of each layer \n",
        "        # use the ReLU function for all hidden layers and the sigmoid for the ultimate layer\n",
        "        h1 = \n",
        "        h2 = \n",
        "        prediction = \n",
        "\n",
        "        # compute the mean squared loss\n",
        "        loss = \n",
        "\n",
        "        # multiply how much we missed (derivative of the loss function) by the \n",
        "        # slope of the sigmoid (derivative) at the values in the ultimate layer\n",
        "        loss_delta = \n",
        "        \n",
        "        # backpropagate above quantity for the second layer error\n",
        "        h2_error = \n",
        "        h2_delta = \n",
        "\n",
        "        # backpropagate one more layer down\n",
        "        h1_error = \n",
        "        h1_delta = \n",
        "\n",
        "        # update the weights\n",
        "        W2 += \n",
        "        W1 += \n",
        "        W0 += \n",
        "\n",
        "    print(\"Error:\" + str(np.mean(loss)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXvbxjqXmoxt",
        "colab_type": "text"
      },
      "source": [
        "### Accuracy\n",
        "Let's again write a function to calculate the models accuracy on the classification task and evaluate it for train and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBEbdCPWMBGy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_acc(x, y):\n",
        "    acc = 0.0\n",
        "    \n",
        "    # do forward propagation (you can copy paste your above code)\n",
        "    h1 = \n",
        "    h2 = \n",
        "    prediction = \n",
        "\n",
        "    # get the most likely class index of the prediction\n",
        "    predicted_label = \n",
        "    \n",
        "    # calculate the accuracy by comparing with the ground truth labels\n",
        "    acc = np.sum(predicted_label == y) / len(x)\n",
        "            \n",
        "    return acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1k2Q48dwMRso",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_acc = eval_acc(Dataset.train_x, Dataset.train_y)\n",
        "val_acc = eval_acc(Dataset.val_x, Dataset.val_y)\n",
        "\n",
        "print(\"Training accuracy: \", train_acc)    \n",
        "print(\"Validation accuracy: \", val_acc)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AMzvTbzq0YE",
        "colab_type": "text"
      },
      "source": [
        "Even with a simple MLP we can achieve accuracies greater than 75%. For reference: 10% is the chance if one were to random guess. We will later see that more advanced neural networks that can better take into account spatial information, such as Convolutional Neural Networks (CNN) will do even much better than that. \n",
        "\n",
        "You can check how well you are doing on FashionMNIST by checking the online leaderboard: http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBGp0ezhp25c",
        "colab_type": "text"
      },
      "source": [
        "### Monitoring and confusion matrices\n",
        "\n",
        "Instead of simply looking at the overall accuracy, we could monitor a confusion matrix. In a confusion matrix both axes of the matrix represent the classes of the classification task and each row describes the class that our neural network predicted for each given label.  Using this information we can determine which class is particularly difficult or easy, which class often gets predicted as false positive or which ones get missed. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9yEpiLooRWG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def calculate_confusion(x, y):\n",
        "    # forward propagation (you can copy paste your above code)\n",
        "    h1 = \n",
        "    h2 = \n",
        "    prediction = \n",
        "    \n",
        "    # get the most likely class index of the prediction\n",
        "    predicted_label = \n",
        "\n",
        "    # use sklearns confusion matrix function\n",
        "    confusion = confusion_matrix(y, predicted_label)\n",
        "    \n",
        "    # normalize the confusion matrix\n",
        "    confusion = confusion.astype('float') / confusion.sum(axis=1)[:, np.newaxis]\n",
        "    \n",
        "    return confusion\n",
        "\n",
        "def visualize_confusion(matrix, class_dict):\n",
        "    \"\"\"\n",
        "    Visualization of confusion matrix\n",
        "\n",
        "    Parameters:\n",
        "        matrix (numpy.array): Square-shaped array of size class x class.\n",
        "            Should specify cross-class accuracies/confusion in percent\n",
        "            values (range 0-1).\n",
        "        class_dict (dict): Dictionary specifying class names as keys and\n",
        "            corresponding integer labels/targets as values.\n",
        "    \"\"\"\n",
        "\n",
        "    all_categories = sorted(class_dict, key=class_dict.get)\n",
        "\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(matrix)\n",
        "    fig.colorbar(cax,\n",
        "                 boundaries=[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticklabels([''] + all_categories, rotation=90)\n",
        "    ax.set_yticklabels([''] + all_categories)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qps3-3rqo65O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conf = calculate_confusion(Dataset.val_x, Dataset.val_y)\n",
        "visualize_confusion(conf, Dataset.class_to_idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1HsaCPfmPjP",
        "colab_type": "text"
      },
      "source": [
        "## Extending what we have\n",
        "\n",
        "* 1.) The used error metric is linear and thus makes it harder to learn for the model when high accuracies are reached (because error and output values are very close and lead to small gradients). Implement a Cross-entropy criterion based on the log-softmax function followed by the negative log-likelihood instead. Why is this a better choice for a multi-class classification problem? \n",
        "* 2.) The above example only used the training set and monitored the training error. This is generally a bad idea because we have no notion of over-fitting. Use the given validation set to monitor your error and decide a suitable stopping point. \n"
      ]
    }
  ]
}