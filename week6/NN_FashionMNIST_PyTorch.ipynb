{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN_FashionMNIST_PyTorch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrtnMndt/mlpr19/blob/master/week6/NN_FashionMNIST_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxZXfBj-ZwXG",
        "colab_type": "text"
      },
      "source": [
        "# Neural Networks for Fashion MNIST in PyTorch\n",
        "We will extend our previous MLP from scratch example by re-implementing the same content in PyTorch. This may seem like a tour-de-force, but will show just exactly how much of the complicated underlying implementation is abstracted away from the user in modern Deep Learning frameworks. We will then proceed to implement a simple convolutional neural network (CNN). \n",
        "\n",
        "Luckily, PyTorch is already installed by default in Colab. We will install one auxiliary package called torchnet: https://github.com/pytorch/tnt though, which we will use for confusion matrices. \n",
        "\n",
        "Before starting the notebook you should make sure that your runtime uses GPU acceleration. You can find the corresponding option under *runtime* and then *change runtime type*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHbvMRv4aHMj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install torchnet "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fR1gvRs2ukp",
        "colab_type": "text"
      },
      "source": [
        "As always we will import numpy as we will still use it for our dataloader. We now also import PyTorch (simply called torch when importing) and particularly its neural network specific part *nn*. To be on the safe side you can also print Colab's pre-installed version of PyTorch and check if it corresponds to the most recent version (or alternatively update it)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7p-P51_uaXIi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "print(torch.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "NB7ZruPuZwXH",
        "colab_type": "text"
      },
      "source": [
        "### Dataset class extended to use directly in PyTorch\n",
        "We can basically take our given dataset loader from our previous MLP from scratch FashionMNIST example and use it almost as is.\n",
        "\n",
        "There is one modification that we absolutely have to make which is converting the numpy arrays to torch tensors.\n",
        "Below, we will have to use the function *torch.from_numpy()* for this purpose. \n",
        "\n",
        "Two additional features we can add is the use of PyTorch dataset and dataloader structures that are very convenient to use and highly efficient. \n",
        "These are called *torch.utils.data.TensorDataset* and *torch.utils.data.DataLoader* and allow for the use of a multi-threaded mini-batch dataset loader. In contrast to storing the entire dataset in our memory, this data loader allows us to only load and return the current mini-batch and e.g. store the rest of the dataset in terms of paths only. Although we can just load the entire dataset at once in our simple example (in fact we still do when loading it into Numpy the first time), this is particularly useful for large datasets thart do not fit our memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UhzkhiZZwXI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import struct\n",
        "import gzip\n",
        "import errno\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "\n",
        "class FashionMNIST:\n",
        "    \"\"\"\n",
        "    Fashion MNIST dataset featuring gray-scale 28x28 images of\n",
        "    fashion items belonging to ten different classes.\n",
        "    Dataloader adapted from MNIST.\n",
        "    We do not define __getitem__ and __len__ in this class\n",
        "    as we are using torch.utils.data.TensorDataSet which\n",
        "    already implements these methods.\n",
        "\n",
        "    Parameters:\n",
        "        args (dict): Dictionary of (command line) arguments.\n",
        "            Needs to contain batch_size (int) and workers(int).\n",
        "        is_gpu (bool): True if CUDA is enabled.\n",
        "            Sets value of pin_memory in DataLoader.\n",
        "\n",
        "    Attributes:\n",
        "        trainset (torch.utils.data.TensorDataset): Training set wrapper.\n",
        "        valset (torch.utils.data.TensorDataset): Validation set wrapper.\n",
        "        train_loader (torch.utils.data.DataLoader): Training set loader with shuffling.\n",
        "        val_loader (torch.utils.data.DataLoader): Validation set loader.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, is_gpu, batch_size, workers):\n",
        "        self.path = os.path.expanduser('datasets/FashionMNIST')\n",
        "        self.__download()\n",
        "\n",
        "        self.trainset, self.valset = self.get_dataset()\n",
        "\n",
        "        self.train_loader, self.val_loader = self.get_dataset_loader(batch_size, workers, is_gpu)\n",
        "\n",
        "        self.val_loader.dataset.class_to_idx = {'T-shirt/top': 0,\n",
        "                                                'Trouser': 1,\n",
        "                                                'Pullover': 2,\n",
        "                                                'Dress': 3,\n",
        "                                                'Coat': 4,\n",
        "                                                'Sandal': 5,\n",
        "                                                'Shirt': 6,\n",
        "                                                'Sneaker': 7,\n",
        "                                                'Bag': 8,\n",
        "                                                'Ankle boot': 9}\n",
        "\n",
        "    def __check_exists(self):\n",
        "        \"\"\"\n",
        "        Checks if dataset has already been downloaded\n",
        "\n",
        "        Returns:\n",
        "             bool: True if downloaded dataset has been found\n",
        "        \"\"\"\n",
        "\n",
        "        return os.path.exists(os.path.join(self.path, 'train-images-idx3-ubyte.gz')) and \\\n",
        "               os.path.exists(os.path.join(self.path, 'train-labels-idx1-ubyte.gz')) and \\\n",
        "               os.path.exists(os.path.join(self.path, 't10k-images-idx3-ubyte.gz')) and \\\n",
        "               os.path.exists(os.path.join(self.path, 't10k-labels-idx1-ubyte.gz'))\n",
        "\n",
        "    def __download(self):\n",
        "        \"\"\"\n",
        "        Downloads the Fashion-MNIST dataset from the web if dataset\n",
        "        hasn't already been downloaded.\n",
        "        \"\"\"\n",
        "\n",
        "        from six.moves import urllib\n",
        "\n",
        "        if self.__check_exists():\n",
        "            return\n",
        "\n",
        "        print(\"Downloading FashionMNIST dataset\")\n",
        "        urls = [\n",
        "            'https://cdn.rawgit.com/zalandoresearch/fashion-mnist/ed8e4f3b/data/fashion/train-images-idx3-ubyte.gz',\n",
        "            'https://cdn.rawgit.com/zalandoresearch/fashion-mnist/ed8e4f3b/data/fashion/train-labels-idx1-ubyte.gz',\n",
        "            'https://cdn.rawgit.com/zalandoresearch/fashion-mnist/ed8e4f3b/data/fashion/t10k-images-idx3-ubyte.gz',\n",
        "            'https://cdn.rawgit.com/zalandoresearch/fashion-mnist/ed8e4f3b/data/fashion/t10k-labels-idx1-ubyte.gz',\n",
        "        ]\n",
        "\n",
        "        # download files\n",
        "        try:\n",
        "            os.makedirs(self.path)\n",
        "        except OSError as e:\n",
        "            if e.errno == errno.EEXIST:\n",
        "                pass\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "        for url in urls:\n",
        "            print('Downloading ' + url)\n",
        "            data = urllib.request.urlopen(url)\n",
        "            filename = url.rpartition('/')[2]\n",
        "            file_path = os.path.join(self.path, filename)\n",
        "            with open(file_path, 'wb') as f:\n",
        "                f.write(data.read())\n",
        "\n",
        "        print('Done!')\n",
        "\n",
        "    def __get_fashion_mnist(self, path, kind='train'):\n",
        "        \"\"\"\n",
        "        Load Fashion-MNIST data\n",
        "\n",
        "        Parameters:\n",
        "            path (str): Base directory path containing .gz files for\n",
        "                the Fashion-MNIST dataset\n",
        "            kind (str): Accepted types are 'train' and 't10k' for\n",
        "                training and validation set stored in .gz files\n",
        "\n",
        "        Returns:\n",
        "            numpy.array: images, labels\n",
        "        \"\"\"\n",
        "\n",
        "        labels_path = os.path.join(path,\n",
        "                                   '%s-labels-idx1-ubyte.gz'\n",
        "                                   % kind)\n",
        "        images_path = os.path.join(path,\n",
        "                                   '%s-images-idx3-ubyte.gz'\n",
        "                                   % kind)\n",
        "\n",
        "        with gzip.open(labels_path, 'rb') as lbpath:\n",
        "            struct.unpack('>II', lbpath.read(8))\n",
        "            labels = np.frombuffer(lbpath.read(), dtype=np.uint8)\n",
        "\n",
        "        with gzip.open(images_path, 'rb') as imgpath:\n",
        "            struct.unpack(\">IIII\", imgpath.read(16))\n",
        "            images = np.frombuffer(imgpath.read(), dtype=np.uint8).reshape(len(labels), 784)\n",
        "\n",
        "        return images, labels\n",
        "\n",
        "    def get_dataset(self):\n",
        "        \"\"\"\n",
        "        Loads and wraps training and validation datasets\n",
        "\n",
        "        Returns:\n",
        "             torch.utils.data.TensorDataset: trainset, valset\n",
        "        \"\"\"\n",
        "\n",
        "        x_train, y_train = self.__get_fashion_mnist(self.path, kind='train')\n",
        "        x_val, y_val = self.__get_fashion_mnist(self.path, kind='t10k')\n",
        "\n",
        "        # This is new with respect to our previous data loader\n",
        "        # convert to torch tensors in range [0, 1]\n",
        "        # after conversion (before normalization) you need to cast the training data to float\n",
        "        # and the labels to long (integers)\n",
        "        x_train = \n",
        "        y_train = \n",
        "        x_val = \n",
        "        y_val = \n",
        "\n",
        "        # resize flattened array of images for input to a CNN\n",
        "        # we use the in-place variant of the resize function here\n",
        "        x_train.resize_(x_train.size(0), 1, 28, 28)\n",
        "        x_val.resize_(x_val.size(0), 1, 28, 28)\n",
        "\n",
        "        # TensorDataset wrapper\n",
        "        trainset = torch.utils.data.TensorDataset(x_train, y_train)\n",
        "        valset = torch.utils.data.TensorDataset(x_val, y_val)\n",
        "\n",
        "        return trainset, valset\n",
        "\n",
        "    def get_dataset_loader(self, batch_size, workers, is_gpu):\n",
        "        \"\"\"\n",
        "        Defines the dataset loader for wrapped dataset\n",
        "\n",
        "        Parameters:\n",
        "            batch_size (int): Defines the batch size in data loader\n",
        "            workers (int): Number of parallel threads to be used by data loader\n",
        "            is_gpu (bool): True if CUDA is enabled so pin_memory is set to True\n",
        "\n",
        "        Returns:\n",
        "             torch.utils.data.TensorDataset: trainset, valset\n",
        "        \"\"\"\n",
        "\n",
        "        # multi-threaded data loaders\n",
        "        train_loader = torch.utils.data.DataLoader(self.trainset, batch_size=batch_size, shuffle=True,\n",
        "                                                   num_workers=workers, pin_memory=is_gpu, sampler=None)\n",
        "        test_loader = torch.utils.data.DataLoader(self.valset, batch_size=batch_size, shuffle=True,\n",
        "                                                  num_workers=workers, pin_memory=is_gpu, sampler=None)\n",
        "\n",
        "        return train_loader, test_loader\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuIk7hjj5Xbl",
        "colab_type": "text"
      },
      "source": [
        "Let's load the data and set the device to use. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIz2xlhpZwXM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set a boolean flag that indicates whether a cuda capable GPU is available \n",
        "# we will need this for transferring our tensors to the device and \n",
        "# for persistent memory in the data loader\n",
        "is_gpu = torch.cuda.is_available()\n",
        "print(\"GPU is available:\", is_gpu)\n",
        "print(\"If you are receiving False, try setting your runtime to GPU\")\n",
        "\n",
        "# set the device to cuda if a GPU is available\n",
        "device = torch.device(\"cuda\" if is_gpu else \"cpu\")\n",
        "\n",
        "# in contrast to our MLP from scratch notebook, we need to set the batch size already now\n",
        "# this is because our data loader now requires it.\n",
        "batch_size = 128\n",
        "# we also set the amount of workers, i.e. parallel threads to use in our data loader\n",
        "workers = 4\n",
        "\n",
        "# We can now instantiate our dataset class \n",
        "dataset = "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKpmq-3VZwXP",
        "colab_type": "text"
      },
      "source": [
        "### The MLP model in PyTorch\n",
        "We now show how to implement a 2 hidden layer MLP in PyTorch. \n",
        "\n",
        "Suitable hidden-layer sizes for this task could be 100 and 100, like in our last notebook. \n",
        "Because we are using an optimized GPU implementation, you are welcome and should try larger sizes to see the impact of neural network size (capacity) on our task!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JXFkMigZwXR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, img_size, num_classes):\n",
        "        super(MLP, self).__init__()\n",
        "        \n",
        "        self.img_size = img_size\n",
        "        \n",
        "        # we can optionally set the \"bias=False\" in our layers (like in previous notebook)\n",
        "        self.fc1 = \n",
        "        self.fc2 = \n",
        "        self.fc3 = \n",
        "\n",
        "    def forward(self, x):\n",
        "        # The view flattens the data to a vector (the representation needed by the MLP)\n",
        "        x = x.view(-1, self.img_size)\n",
        "        \n",
        "        x = # apply first layer and activatiom\n",
        "        x = # apply second layer and activation\n",
        "        x = # apply last linear layer (no activation)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vxw9nHKPZwXT",
        "colab_type": "text"
      },
      "source": [
        "### Defining optimization criterion and optimizer\n",
        "A good baseline is a Cross Entropy loss (that combines a logarithmic Softmax + negative log-likelihood) and a stochastic gradient descent (SGD) algorithm with a baseline learning rate of 0.01. \n",
        "The Softmax function: https://en.wikipedia.org/wiki/Softmax_function is similar to the Sigmoid function, but is a normalized exponential and thus normalizes the probability of the output. In contrast to the Sigmoid unit that just gives out values in the range of 0-1 for each output unit, the Softmax function outputs values that are normalized to 1 across the entire range of all outputs. \n",
        "\n",
        "\n",
        "If we want to we can use additional momenta or regularization terms (such as L2 - Tikhonov regularization commonly reffered to as weight-decay in ML). The respective optimizer parameters are called *momentum* and *weight_decay*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHI6LGtuZwXU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define optimizer and loss function (criterion)\n",
        "img_size = 28 * 28\n",
        "num_classes = 10\n",
        "\n",
        "# create an instance of the MLP and transfer the model to the device.\n",
        "# Note that we do not necessarily need any custom weight initialization as PyTorch\n",
        "# already uses the initialization schemes that we have previously learned about internally. \n",
        "model = MLP(img_size, num_classes).to(device)\n",
        "# we can also print the model architecture\n",
        "print(model)\n",
        "\n",
        "# set the loss function\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# we can use advanced stochastic gradient descent algorithms \n",
        "# with regularization (weight-decay) or momentum\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01,\n",
        "                            momentum=0.9,\n",
        "                            weight_decay=5e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JI1Bei8DZwXW",
        "colab_type": "text"
      },
      "source": [
        "### Monitoring and calculating accuracy\n",
        "We add a convenience class to keep track and average concepts such as processing or data loading speeds, losses and accuracies. For this we need to define a function to define accuracy, which could be based on the absolute accuracy, or top-1 accuracy. Often times in Machine Learning other metrics are employed. For example, in the ImageNet ILSVRC challenge with a classification problem containing 1000 classes, it is common to report the top-5 accuracy. Here a prediction is counted as accurate if the correct class lies within the top-5 most likely output classes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajy_8OckZwXX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"\n",
        "    Computes and stores the average and current value\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"\n",
        "    Evaluates a model's top k accuracy\n",
        "\n",
        "    Parameters:\n",
        "        output (torch.autograd.Variable): model output\n",
        "        target (torch.autograd.Variable): ground-truths/labels\n",
        "        topk (list): list of integers specifying top-k precisions\n",
        "            to be computed\n",
        "\n",
        "    Returns:\n",
        "        float: percentage of correct predictions\n",
        "    \"\"\"\n",
        "\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].view(-1).float().sum(0)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIqY9nJOZwXa",
        "colab_type": "text"
      },
      "source": [
        "### Training function (sometimes referred to as \"hook\")\n",
        "The training function needs to loop through the entire dataset in steps of mini-batches (for SGD). For each mini-batch the output of the model and losses are calculated and a *backward* pass is done to calculate gradients and an *optimizer step* is done in order to do the respective update to the model's weights. This is similar to our former notebook where we first calculate the errors/deltas for every layer and then apply the weight updates at the end.\n",
        "\n",
        "When the entire dataset has been processed once, one epoch of the training has been conducted. It is common to shuffle the dataset after each epoch. In contrast to our previous notebook from scratch, in this implementation this is handled by the \"sampler\" of the dataset loader. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hyNEtLtZwXa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(train_loader, model, criterion, optimizer, device):\n",
        "    \"\"\"\n",
        "    Trains/updates the model for one epoch on the training dataset.\n",
        "\n",
        "    Parameters:\n",
        "        train_loader (torch.utils.data.DataLoader): The trainset dataloader\n",
        "        model (torch.nn.module): Model to be trained\n",
        "        criterion (torch.nn.criterion): Loss function\n",
        "        optimizer (torch.optim.optimizer): optimizer instance like SGD or Adam\n",
        "        device (string): cuda or cpu\n",
        "    \"\"\"\n",
        "\n",
        "    # create instances of the average meter to track losses and accuracies\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "\n",
        "    # iterate through the dataset loader\n",
        "    for i, (inp, target) in enumerate(train_loader):\n",
        "        # transfer inputs and targets to the GPU (if it is available)\n",
        "        inp = \n",
        "        target = \n",
        "\n",
        "        # compute output, i.e. the model forward\n",
        "        output = \n",
        "        \n",
        "        # calculate the loss\n",
        "        loss = \n",
        "\n",
        "        # measure accuracy and record loss and accuracy\n",
        "        prec1, prec5 = accuracy(output, target, topk=(1, 5))\n",
        "        losses.update(loss.item(), inp.size(0))\n",
        "        top1.update(prec1.item(), inp.size(0))\n",
        "\n",
        "        # compute gradient and do the SGD step\n",
        "        # we reset the optimizer with zero_grad to \"flush\" former gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        loss # backpropagate the loss here \n",
        "        optimizer # do the optimizer step here\n",
        "\n",
        "        # print the loss every 100 mini-batches\n",
        "        if i % 100 == 0:\n",
        "            print('Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
        "                   loss=losses, top1=top1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YpNuUY0ZwXd",
        "colab_type": "text"
      },
      "source": [
        "### Validation function\n",
        "Validation is similar to the training loop, but on a separate dataset with the exception that no update to the weights is performed. This way we can monitor the generalization ability of our model and check whether it is overfitting (memorizing) the training dataset.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zihygz7tZwXd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchnet import meter\n",
        "\n",
        "def validate(val_loader, model, criterion, device):\n",
        "    \"\"\"\n",
        "    Evaluates/validates the model\n",
        "\n",
        "    Parameters:\n",
        "        val_loader (torch.utils.data.DataLoader): The validation or testset dataloader\n",
        "        model (torch.nn.module): Model to be evaluated/validated\n",
        "        criterion (torch.nn.criterion): Loss function\n",
        "        device (string): cuda or cpu\n",
        "    \"\"\"\n",
        "\n",
        "    # create instances of the average meter to track losses and accuracies\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "\n",
        "    confusion = meter.ConfusionMeter(len(val_loader.dataset.class_to_idx))\n",
        "\n",
        "    # switch to evaluate mode \n",
        "    # (this would be important for e.g. dropout where stochasticity shouldn't be applied during testing)\n",
        "    model.eval()\n",
        "\n",
        "    # avoid computation of gradients and necessary storing of intermediate layer activations\n",
        "    with torch.no_grad():\n",
        "        # iterate through the dataset loader\n",
        "        for i, (inp, target) in enumerate(val_loader):\n",
        "            # transfer to device\n",
        "            inp = \n",
        "            target = \n",
        "\n",
        "            # compute output\n",
        "            output =\n",
        "\n",
        "            # compute loss\n",
        "            loss =\n",
        "\n",
        "            # measure accuracy and record loss and accuracy\n",
        "            prec1, _ = accuracy(output, target, topk=(1, 5))\n",
        "            losses.update(loss.item(), inp.size(0))\n",
        "            top1.update(prec1.item(), inp.size(0))\n",
        "\n",
        "            # add to confusion matrix\n",
        "            confusion.add(output.data, target)\n",
        "\n",
        "    print(' * Validation accuracy: Prec@1 {top1.avg:.3f} '.format(top1=top1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW-sqbUaZwXi",
        "colab_type": "text"
      },
      "source": [
        "### Running the training of the model\n",
        "Let's optimize this model for 20 epochs and check at every epoch how we are doing on our validation set. \n",
        "\n",
        "Depending on your model definition and optimizer you might experience over-fitting!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFvFnowLZwXj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_epochs = 20\n",
        "for epoch in range(total_epochs):\n",
        "    print(\"EPOCH:\", epoch + 1)\n",
        "    print(\"TRAIN\")\n",
        "    # call the train function\n",
        "    train(...)\n",
        "    print(\"VALIDATION\")\n",
        "    # call the validation function\n",
        "    validate(...)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "kldGeVaXZwXo",
        "colab_type": "text"
      },
      "source": [
        "### Moving from MLP to CNN\n",
        "Now that we have seen how our two-hidden layer MLP performs, let's see how we can move on to a convolutional neural network (CNN). The advantage of a CNN is that the we no longer have an all-to-all connectivity structure between layers, but rather take a look at local (2-D or even 3-D) neighborhoods. This spatial (or even temporal) filter is then convolved over the whole input (here an image) by \"sharing the weights\" to every position. The outcome is typically referred to as a feature map and in order to check for multiple features we apply a set of such filters in parallel.  We will see how these effects improve our accuracy in contrast to our MLP. \n",
        "\n",
        "Let us see how to build a CNN with 2 layers with a fully-connected classifier on top and included pooling layers after every convolution. These layers generally subsample the input and introduce translation invariance (to an extent). The network should again have rectified linear units for activation functions and end on a fully-connected linear layer to the amount of classes.\n",
        "\n",
        "    1. Define two convolution layers \"nn.Conv2d\" with 5 x 5 filters. Good starting values for amount of filters/features can be 64 in the first and 128 in the second layer.\n",
        "    2. Convolutions should be followed by ReLU activations. You can apply the activations in the definition of the forward pass with the functional package and \"F.relu\"\n",
        "    3. Each conv + act block should be followed by a 2 x 2 max pooling \"nn.MaxPool2d\" with stride 2.\n",
        "    4. You will need to calculate the remaining feature * spatial dimensionality to flatten the convolutional output to feed it to the last fully-connected layer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4bPPd5uZwXo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CNN, self).__init__()\n",
        "        \n",
        "        self.conv1 =  # input features, output features, kernel size\n",
        "        self.mp1 =  # kernel size, stride\n",
        "        \n",
        "        self.conv2 =  # input features, output features, kernel size\n",
        "        self.mp2 =  # kernel size, stride\n",
        "        \n",
        "        self.fc =  # 4x4 is the remaining spatial resolution here\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Conv + ReLU + max pooling for two layers\n",
        "        x = \n",
        "        x = \n",
        "        # The view flattens the output to a vector (the representation needed by the classifier)\n",
        "        x = x.view(-1, your_determined_size_here)\n",
        "        # apply fully-connected linear layer\n",
        "        x = \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWBpW7QEZwXq",
        "colab_type": "text"
      },
      "source": [
        "### Constructing and running the CNN\n",
        "Let's create an instance of our CNN model and optimize it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CfyFWGXZwXr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create CNN model instance\n",
        "model = \n",
        "print(model)\n",
        "\n",
        "# again, define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01,\n",
        "                            momentum=0.9,\n",
        "                            weight_decay=5e-4)\n",
        "\n",
        "# optimize\n",
        "total_epochs = 20\n",
        "for epoch in range(total_epochs):\n",
        "    print(\"EPOCH:\", epoch + 1)\n",
        "    print(\"TRAIN\")\n",
        "    train(...)\n",
        "    print(\"VALIDATION\")\n",
        "    validate(...)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iwniQFSZwXu",
        "colab_type": "text"
      },
      "source": [
        "We can see that by changing to a CNN for images we have gained a couple percent accuracy already. If you want to play around with this example you will be able to gain even more by modifying the network to include regularization methods such as dropout, augmenting or preprocessing your data, constructing larger and deeper models and finding better hyperparameters such as learning rates or mini-batch sizes.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDZzpmoGZwXv",
        "colab_type": "text"
      },
      "source": [
        "### How well did the model do?\n",
        "In Machine Learning research it is crucial to compare and contrast a model to other researchers implementations. Many of the current Machine Learning datasets are posed as benchmarks where results are rigorously tracked in order to examine the efficiency and efficacy of a model or algorithm proposition.\n",
        "\n",
        "For the fashion MNIST dataset you can check how well both of your models (from scratch and in PyTorch) perform here:\n",
        "http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/#\n",
        "\n",
        "Do keep in mind that in order to analyze the usefulness of a method one should always compare and contrast on a variety of different datasets with varying task and complexity."
      ]
    }
  ]
}